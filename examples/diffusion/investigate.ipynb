{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a722ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "act_rs = torch.load(\"final_result/nvfp4_rs/extractions/activations_during_generation.pt\", weights_only=False)\n",
    "w_rs_af = torch.load(\"final_result/nvfp4_rs/extractions/weights_after_smooth.pt\", weights_only=False)\n",
    "\n",
    "act_org = torch.load(\"final_result/nvfp4/extractions/activations_during_generation.pt\", weights_only=False)\n",
    "w_org = torch.load(\"final_result/nvfp4/extractions/weights_before_smooth.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b3cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_rs_af.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33565b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_scale = torch.load(\"runs/diffusion/cache/quant/qdiff.128/smooth/w.4-x.4-y.16-w.4/w.sfp4_e2m1_all-x.sfp4_e2m1_all-y.bf16-w.sint4/w.v16.sfp8_e4m3_nan.tsnr.bf16-x.v16.sfp8_e4m3_nan-y.tnsr.bf16-w.v64.bf16/w.skip.[e+rs+rtp+s+tpi+tpo]-x.skip.[e+rs+rtp+s+tan+tn+tpi+tpo]-y.skip.[]-w.include.[tan+tn]/smooth.proj.OutputsError.GridSearch.Layer.d2.en1.sn1/smooth.proj.[a.AbsMax.b.AbsMax]/smooth.proj.g20.a0.bn2/smooth.proj..rev.nf/smooth.proj.skip.[rc+tan+tn]/flux.1-schnell.pt\", weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = 'transformer_blocks.11.attn.to_q'\n",
    "w = w_org[target_layer]['weight']  # [num_samples, num_timesteps, batch_size, hidden_dim, hidden_dim]\n",
    "rs_w = w_rs_af[target_layer]['weight']  # [num_samples, num_timesteps, batch_size, hidden_dim, hidden_dim]\n",
    "act = {\n",
    "    \"MJHQ\" : {\n",
    "        \"org\" : {\n",
    "            t : {\"input_acts\" : [], \"output_acts\" : []} for t in range(4)\n",
    "        },\n",
    "        \"reverse\" : {\n",
    "            t : {\"input_acts\" : [], \"output_acts\" : []} for t in range(4)\n",
    "        },\n",
    "    },\n",
    "    \"sDCI\" : {\n",
    "        \"org\" : {\n",
    "            t : {\"input_acts\" : [], \"output_acts\" : []} for t in range(4)\n",
    "        },\n",
    "        \"reverse\" : {\n",
    "            t : {\"input_acts\" : [], \"output_acts\" : []} for t in range(4)\n",
    "        },\n",
    "    }\n",
    "}\n",
    "for i, (ipt, opt) in enumerate(zip(act_org[target_layer+\".input\"][-1], act_org[target_layer+\".output\"][-1])):\n",
    "    if i < 64:\n",
    "        dataset = \"MJHQ\"\n",
    "    else:\n",
    "        dataset = \"sDCI\"\n",
    "    timestep = i % 4\n",
    "    act[dataset][\"org\"][timestep][\"input_acts\"].append(ipt)\n",
    "    act[dataset][\"org\"][timestep][\"output_acts\"].append(opt)\n",
    "for i, (ipt, opt) in enumerate(zip(act_rs[target_layer+\".input\"][-1], act_rs[target_layer+\".output\"][-1])):\n",
    "    if i < 64:\n",
    "        dataset = \"MJHQ\"\n",
    "    else:\n",
    "        dataset = \"sDCI\"\n",
    "    timestep = i % 4\n",
    "    act[dataset][\"reverse\"][timestep][\"input_acts\"].append(ipt)\n",
    "    act[dataset][\"reverse\"][timestep][\"output_acts\"].append(opt)\n",
    "for dataset in act.keys():\n",
    "    for mode in act[dataset].keys():\n",
    "        for t in act[dataset][mode].keys():\n",
    "            act[dataset][mode][t][\"input_acts\"] = torch.stack(act[dataset][mode][t][\"input_acts\"], dim=0)\n",
    "            act[dataset][mode][t][\"output_acts\"] = torch.stack(act[dataset][mode][t][\"output_acts\"], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e1ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"weights_before_smooth\" : w,\n",
    "    \"weights_after_smooth\" : rs_w,\n",
    "    \"activations\" : act,\n",
    "}, \"investigate_activations_weights.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2036fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = torch.load(\"investigate_activations_weights.pt\", weights_only=False)\n",
    "w = cache[\"weights_before_smooth\"]\n",
    "rs_w = cache[\"weights_after_smooth\"]\n",
    "act = cache[\"activations\"]\n",
    "smooth_scale = torch.load(\"runs/diffusion/cache/quant/qdiff.128/smooth/w.4-x.4-y.16-w.4/w.sfp4_e2m1_all-x.sfp4_e2m1_all-y.bf16-w.sint4/w.v16.sfp8_e4m3_nan.tsnr.bf16-x.v16.sfp8_e4m3_nan-y.tnsr.bf16-w.v64.bf16/w.skip.[e+rs+rtp+s+tpi+tpo]-x.skip.[e+rs+rtp+s+tan+tn+tpi+tpo]-y.skip.[]-w.include.[tan+tn]/smooth.proj.OutputsError.GridSearch.Layer.d2.en1.sn1/smooth.proj.[a.AbsMax.b.AbsMax]/smooth.proj.g20.a0.bn2/smooth.proj..rev.nf/smooth.proj.skip.[rc+tan+tn]/flux.1-schnell.pt\", weights_only=False)\n",
    "smooth_scale = smooth_scale['transformer_blocks.11.attn.to_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a45029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def lzs(x: torch.Tensor, bits: int = 4) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Real per-block LZS (shared flag per block).\n",
    "    Expects x shaped [..., block_size]. Computes a single flag per block from max MSB in the block.\n",
    "\n",
    "    For bits=4, we keep a 3-bit mantissa [0..7] and a shared power-of-two shift per block.\n",
    "    \"\"\"\n",
    "    # sign + magnitude\n",
    "    sign = torch.sign(x)\n",
    "    a = torch.abs(x).clamp_max_(127)  # keep in INT8-like magnitude range\n",
    "\n",
    "    # MSB per element using frexp: MSB = exp for a>0 else 0\n",
    "    _, exp = torch.frexp(a)\n",
    "    msb = torch.where(a > 0, exp, torch.zeros_like(exp))  # same shape as a\n",
    "\n",
    "    # Block-wise max MSB (shared across last dim = block_size)\n",
    "    msb_blk = msb.amax(dim=-1, keepdim=True)\n",
    "\n",
    "    # Shared FLAG per block: max(MSB) - 3, clamped at 0\n",
    "    flag = (msb_blk - 3).clamp_min_(0)\n",
    "\n",
    "    # scale = 2^flag (shared per block)\n",
    "    scale = torch.ldexp(torch.ones_like(a), flag)\n",
    "\n",
    "    # quantize each element using shared scale\n",
    "    q = torch.round(a / scale).clamp_(0, 7) * scale\n",
    "    return sign * q\n",
    "\n",
    "@torch.no_grad()\n",
    "def quartz(\n",
    "    tensor: torch.Tensor,\n",
    "    quant_block_size: int = 32,\n",
    "    lzs_block_size: int = 16,\n",
    "    mode: str = \"mx\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    GPU-efficient QUARTZ-like quantization + LZS + dequant (verification path).\n",
    "    \"\"\"\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor\n",
    "\n",
    "    orig_shape = tensor.shape\n",
    "    device = tensor.device\n",
    "    dtype = tensor.dtype\n",
    "\n",
    "    # Flatten\n",
    "    x = tensor.reshape(-1)\n",
    "\n",
    "    # Pad to multiple of quant_block_size (avoid torch.cat when possible)\n",
    "    n = x.numel()\n",
    "    r = n % quant_block_size\n",
    "    if r != 0:\n",
    "        pad = quant_block_size - r\n",
    "        x_pad = torch.empty(n + pad, device=device, dtype=dtype)\n",
    "        x_pad[:n].copy_(x)\n",
    "        x_pad[n:].zero_()\n",
    "        x = x_pad\n",
    "\n",
    "    # [num_qblocks, quant_block_size]\n",
    "    x = x.view(-1, quant_block_size)\n",
    "\n",
    "    # Per-block scale (keep in FP32 for stability; cast back later)\n",
    "    x_fp = x.float()\n",
    "\n",
    "    if mode == \"mx\":\n",
    "        scales = torch.amax(x_fp.abs(), dim=1, keepdim=True) / 127.0\n",
    "    elif mode == \"lzs\":\n",
    "        # Your original denominator was (2^(8-2)-1)=63 (effectively 7-bit signed w/ extra headroom)\n",
    "        scales = (torch.amax(x_fp, dim=1, keepdim=True) - torch.amin(x_fp, dim=1, keepdim=True)) / 64.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode {mode} for QUARTZ quantization\")\n",
    "\n",
    "    # Guard against zeros (important for all-zero blocks)\n",
    "    scales = scales.clamp_min_(1e-12)\n",
    "\n",
    "    # INT8 quant (kept as float here to match your code path)\n",
    "    q = torch.round(x_fp / scales).clamp_(-128, 127)\n",
    "\n",
    "    # LZS over all lzs blocks in one shot\n",
    "    q = q.view(-1, lzs_block_size)\n",
    "    q = lzs(q, bits=4)\n",
    "    q = q.view(-1, quant_block_size)\n",
    "\n",
    "    # Dequant (verification)\n",
    "    deq = (q * scales).to(dtype)\n",
    "\n",
    "    # Unpad back to original length, reshape\n",
    "    if r != 0:\n",
    "        deq = deq.view(-1)[:n]\n",
    "    return deq.view(orig_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a5a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# FP4 E2M1 quant/dequant (NVFP4 uses E2M1)   [oai_citation:1‡NVIDIA Developer](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def fp4_e2m1_quant_dequant(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Quantize+dequantize to FP4 E2M1 (bias=1) in float, vectorized.\n",
    "    Representable magnitudes: {0, 0.5, 1, 1.5, 2, 3, 4, 6} (and negatives).\n",
    "    Saturates to +/-6.\n",
    "    \"\"\"\n",
    "    sign = torch.sign(x)\n",
    "    a = torch.abs(x)\n",
    "\n",
    "    # Saturate to max representable magnitude (~6)\n",
    "    a = a.clamp_max(6.0)\n",
    "\n",
    "    out = torch.zeros_like(a)\n",
    "\n",
    "    # Subnormal/zero region: representable are 0 and 0.5\n",
    "    # Midpoint thresholds: 0.25 between 0 and 0.5, 0.75 between 0.5 and 1.0\n",
    "    sub = a < 1.0\n",
    "    out = torch.where(sub & (a >= 0.25) & (a < 0.75), torch.full_like(a, 0.5), out)\n",
    "    out = torch.where(sub & (a >= 0.75), torch.ones_like(a), out)  # near 1.0 maps to 1.0\n",
    "\n",
    "    # Normal region (>=1): values are base * {1.0, 1.5}, base in {1,2,4}\n",
    "    normal = a >= 1.0\n",
    "    if normal.any():\n",
    "        # exponent = floor(log2(a)), clamped to [0,2]\n",
    "        exp = torch.floor(torch.log2(a.clamp_min(1e-30))).to(torch.int32)\n",
    "        exp = exp.clamp(0, 2)\n",
    "\n",
    "        base = torch.pow(torch.tensor(2.0, device=a.device, dtype=a.dtype), exp.to(a.dtype))\n",
    "        mant = a / base  # in [1,2)\n",
    "\n",
    "        # 1 mantissa bit => choose 1.0 vs 1.5; threshold at midpoint 1.25\n",
    "        mant_bit = (mant >= 1.25).to(a.dtype)\n",
    "        val = base * (1.0 + 0.5 * mant_bit)\n",
    "\n",
    "        out = torch.where(normal, val, out)\n",
    "\n",
    "    return sign * out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# FP8 E4M3 quant/dequant for block scales   [oai_citation:2‡NVIDIA Developer](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/)\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def fp8_e4m3_quant_dequant(x: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate quant+dequant to FP8 E4M3 in float, vectorized.\n",
    "\n",
    "    This is an emulation intended for NVFP4 block scales (E4M3). NVFP4 uses\n",
    "    an FP8 (E4M3) scale per 16-value block and an FP32 per-tensor scale.  [oai_citation:3‡NVIDIA Developer](https://developer.nvidia.com/blog/introducing-nvfp4-for-efficient-and-accurate-low-precision-inference/)\n",
    "\n",
    "    We clamp to ~448 max magnitude (common E4M3FN max cited in NVFP4 contexts).  [oai_citation:4‡NVIDIA Docs](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html?utm_source=chatgpt.com)\n",
    "    \"\"\"\n",
    "    sign = torch.sign(x)\n",
    "    a = torch.abs(x)\n",
    "\n",
    "    # clamp to finite max (commonly cited as 448 for E4M3 scales in NVFP4 tooling)\n",
    "    a = a.clamp_max(448.0)\n",
    "\n",
    "    # Handle zeros\n",
    "    out = torch.zeros_like(a)\n",
    "    nz = a > 0\n",
    "    if not nz.any():\n",
    "        return out\n",
    "\n",
    "    # E4M3: bias=7, mantissa bits=3\n",
    "    bias = 7\n",
    "    m_bits = 3\n",
    "\n",
    "    # Use frexp for stable exponent/mantissa extraction: a = frac * 2^exp, frac in [0.5,1)\n",
    "    frac, exp = torch.frexp(a)  # exp is int-like tensor\n",
    "    # Convert to IEEE-style normalized mantissa in [1,2): mant = 2*frac\n",
    "    mant = frac * 2.0\n",
    "    e = exp - 1  # now floor(log2(a))\n",
    "\n",
    "    # Normal exponent field range for FP8 is limited; emulate a reasonable clamp\n",
    "    # (This is approximate; NVIDIA uses specific FP8 variants.)\n",
    "    e_clamped = e.clamp(-6, 8)  # keeps within range that covers up to ~448\n",
    "\n",
    "    base = torch.ldexp(torch.ones_like(a), e_clamped)  # 2^e\n",
    "    mant_norm = (a / base)  # ~[1,2)\n",
    "\n",
    "    # Quantize mantissa to 3 bits: 1.xxx with 3 fractional bits\n",
    "    mant_q = torch.round((mant_norm - 1.0) * (2**m_bits)) / (2**m_bits) + 1.0\n",
    "\n",
    "    # Handle rounding overflow (e.g., mant_q becomes 2.0)\n",
    "    carry = mant_q >= 2.0\n",
    "    mant_q = torch.where(carry, mant_q * 0.5, mant_q)\n",
    "    e2 = torch.where(carry, e_clamped + 1, e_clamped)\n",
    "\n",
    "    # Reconstruct and clamp again\n",
    "    out_nz = mant_q * torch.ldexp(torch.ones_like(a), e2)\n",
    "    out_nz = out_nz.clamp_max(448.0)\n",
    "\n",
    "    out = torch.where(nz, out_nz, out)\n",
    "    return sign * out\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# NVFP4-like quantization\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def nvfp4(tensor: torch.Tensor, block_size: int = 16, return_meta: bool = False):\n",
    "    \"\"\"\n",
    "    NVFP4-like quantization (emulated) with:\n",
    "      Step 1: per-tensor FP32 scale (global_sf)\n",
    "      Step 2: per-block FP8 E4M3 scale (s_fp8), block_size=16\n",
    "      Step 3: per-element FP4 E2M1 values (q_fp4)\n",
    "\n",
    "    Returns dequantized tensor (verification). Optionally returns (global_sf, s_fp8).\n",
    "    \"\"\"\n",
    "    if tensor.numel() == 0:\n",
    "        return (tensor, None) if return_meta else tensor\n",
    "\n",
    "    orig_shape = tensor.shape\n",
    "    x = tensor.reshape(-1).float()\n",
    "    n = x.numel()\n",
    "\n",
    "    # Pad to multiple of block_size (avoid torch.cat)\n",
    "    r = n % block_size\n",
    "    if r != 0:\n",
    "        pad = block_size - r\n",
    "        x_pad = torch.empty(n + pad, device=x.device, dtype=x.dtype)\n",
    "        x_pad[:n].copy_(x)\n",
    "        x_pad[n:].zero_()\n",
    "        x = x_pad\n",
    "\n",
    "    xb = x.view(-1, block_size)  # [num_blocks, block_size]\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 1: Per-tensor scaling (FP32)\n",
    "    # Choose global_sf so that block scales fit FP8 range (~448) with qTypeMax=6\n",
    "    # TensorRT doc expresses scale_fp8 ~ cast_fp8( amax(block) / (6 * globalSf) ).  [oai_citation:5‡NVIDIA Docs](https://docs.nvidia.com/deeplearning/tensorrt/latest/inference-library/work-quantized-types.html?utm_source=chatgpt.com)\n",
    "    # So set globalSf = max_block_amax / (6 * 448) to keep inside range.\n",
    "    # -------------------------\n",
    "    block_amax = xb.abs().amax(dim=1, keepdim=True)  # [B,1]\n",
    "    max_block_amax = block_amax.max()\n",
    "    denom = 6.0 * 448.0\n",
    "    global_sf = torch.where(max_block_amax > 0, max_block_amax / denom, torch.tensor(1.0, device=x.device))\n",
    "    global_sf = global_sf.clamp_min(1e-12)\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 2: Per-block scaling (FP8 E4M3)\n",
    "    # Unquantized block scale in \"FP8 domain\"\n",
    "    # -------------------------\n",
    "    s = block_amax / (6.0 * global_sf)  # [B,1], ideally <= 448\n",
    "    s_fp8 = fp8_e4m3_quant_dequant(s)   # emulate storing scale in FP8 E4M3\n",
    "\n",
    "    # Avoid divide-by-zero for all-zero blocks\n",
    "    s_fp8 = s_fp8.clamp_min(1e-12)\n",
    "\n",
    "    # -------------------------\n",
    "    # Step 3: Quantize values to FP4 E2M1 under combined scale\n",
    "    # q_fp4 = cast_fp4( x / (global_sf * s_fp8) )\n",
    "    # dequant = global_sf * s_fp8 * deq_fp4(q_fp4)\n",
    "    # -------------------------\n",
    "    y = xb / (global_sf * s_fp8)\n",
    "    yq = fp4_e2m1_quant_dequant(y)\n",
    "    deq = yq * (global_sf * s_fp8)\n",
    "\n",
    "    # Unpad + reshape\n",
    "    deq = deq.view(-1)[:n].view(orig_shape).to(tensor.dtype)\n",
    "\n",
    "    if return_meta:\n",
    "        # Return dequant + (global_sf scalar, per-block fp8 scale shaped [num_blocks,1])\n",
    "        return deq, {\"global_sf\": global_sf.to(tensor.dtype), \"block_scales_fp8\": s_fp8.to(tensor.dtype)}\n",
    "    return deq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10e5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128, 354)\n",
    "x_nvfp4 = nvfp4(x, block_size=16)\n",
    "x_quartz = quartz(x, quant_block_size=32, lzs_block_size=16, mode=\"lzs\")\n",
    "\n",
    "print(\"Error NVFP4: \", torch.norm(x - x_nvfp4, p='fro').item())\n",
    "print(\"Error QUARTZ: \", torch.norm(x - x_quartz, p='fro').item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95916d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "w.min(), w.max(), w.mean(), w.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1603e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_w.min(), rs_w.max(), rs_w.mean(), rs_w.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ca9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = \"MJHQ\"\n",
    "t = 0\n",
    "input_acts_org = act[dset][\"org\"][t][\"input_acts\"]\n",
    "input_acts_rs = act[dset][\"reverse\"][t][\"input_acts\"]\n",
    "output_acts_org = act[dset][\"org\"][t][\"output_acts\"]\n",
    "output_acts_rs = act[dset][\"reverse\"][t][\"output_acts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0768a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_acts_org.min(), input_acts_org.max(), input_acts_org.mean(), input_acts_org.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c280530",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_acts_org.min(), output_acts_org.max(), output_acts_org.mean(), output_acts_org.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202d7dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_acts_rs.min(), input_acts_rs.max(), input_acts_rs.mean(), input_acts_rs.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2c71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_acts_rs.min(), output_acts_rs.max(), output_acts_rs.mean(), output_acts_rs.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bddcae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_scale.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e544cb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8d2a641",
   "metadata": {},
   "source": [
    "### Tensor selection for RS vs non-RS comparison\n",
    "\n",
    "We build `data` from the cached weights/activations and then select the input activations to\n",
    "`transformer_blocks.11.attn.to_q` because that layer's output is exactly `XW` for the same weight\n",
    "matrix `W`. This makes `||XW - Q(X)Q(W)||_F` a direct measure of quantization error.\n",
    "\n",
    "We fix dataset (`MJHQ`), timestep, and batch index to avoid mixing distributions, and we use the\n",
    "same evenly spaced token subset for both modes to isolate the effect of reverse smoothing. The\n",
    "token subsample keeps the matmul tractable while still covering the full sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9191d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"weights\": {\"org\": w, \"rs\": rs_w},\n",
    "    \"activations\": act,\n",
    "}\n",
    "\n",
    "dataset = \"MJHQ\"\n",
    "timestep = 0\n",
    "batch_index = 0\n",
    "token_stride = 32  # 4096 / 32 = 128 tokens\n",
    "\n",
    "x_org_full = data[\"activations\"][dataset][\"org\"][timestep][\"input_acts\"][batch_index, 0]\n",
    "x_rs_full = data[\"activations\"][dataset][\"reverse\"][timestep][\"input_acts\"][batch_index, 0]\n",
    "\n",
    "token_idx = torch.arange(0, x_org_full.shape[0], token_stride)\n",
    "x_org = x_org_full[token_idx].float()\n",
    "x_rs = x_rs_full[token_idx].float()\n",
    "w_org = data[\"weights\"][\"org\"].float()\n",
    "w_rs = data[\"weights\"][\"rs\"].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148f7cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def quartz_lastdim(\n",
    "    tensor: torch.Tensor,\n",
    "    quant_block_size: int = 32,\n",
    "    lzs_block_size: int = 16,\n",
    "    mode: str = \"lzs\",\n",
    ") -> torch.Tensor:\n",
    "    if tensor.numel() == 0:\n",
    "        return tensor\n",
    "    if quant_block_size % lzs_block_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"quant_block_size ({quant_block_size}) must be a multiple of lzs_block_size ({lzs_block_size})\"\n",
    "        )\n",
    "\n",
    "    orig_shape = tensor.shape\n",
    "    x = tensor.reshape(-1, orig_shape[-1])\n",
    "    bsz, width = x.shape\n",
    "\n",
    "    r = width % quant_block_size\n",
    "    if r != 0:\n",
    "        pad = quant_block_size - r\n",
    "        x_pad = torch.empty(bsz, width + pad, device=x.device, dtype=x.dtype)\n",
    "        x_pad[:, :width].copy_(x)\n",
    "        x_pad[:, width:].zero_()\n",
    "        x = x_pad\n",
    "        width = width + pad\n",
    "\n",
    "    x = x.view(bsz, -1, quant_block_size)\n",
    "    x_fp = x.float()\n",
    "\n",
    "    if mode == \"mx\":\n",
    "        scales = x_fp.abs().amax(dim=2, keepdim=True) / 127.0\n",
    "    elif mode == \"lzs\":\n",
    "        scales = (x_fp.amax(dim=2, keepdim=True) - x_fp.amin(dim=2, keepdim=True)) / 64.0\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown mode {mode} for QUARTZ quantization\")\n",
    "\n",
    "    scales = scales.clamp_min_(1e-12)\n",
    "    q = torch.round(x_fp / scales).clamp_(-128, 127)\n",
    "\n",
    "    q = q.view(-1, lzs_block_size)\n",
    "    q = lzs(q, bits=4)\n",
    "    q = q.view(bsz, -1, quant_block_size)\n",
    "\n",
    "    deq = (q * scales).to(tensor.dtype)\n",
    "    if r != 0:\n",
    "        deq = deq.view(bsz, -1)[:, : orig_shape[-1]]\n",
    "    return deq.view(orig_shape)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def quartz_matmul_error(\n",
    "    x: torch.Tensor,\n",
    "    w: torch.Tensor,\n",
    "    group_size: int,\n",
    "    lzs_block_size=None,\n",
    "    mode: str = \"lzs\",\n",
    "    y_ref=None,\n",
    ") -> float:\n",
    "    if lzs_block_size is None:\n",
    "        lzs_block_size = min(16, group_size)\n",
    "    if group_size % lzs_block_size != 0:\n",
    "        raise ValueError(\n",
    "            f\"group_size ({group_size}) must be a multiple of lzs_block_size ({lzs_block_size})\"\n",
    "        )\n",
    "\n",
    "    if y_ref is None:\n",
    "        y_ref = x @ w\n",
    "\n",
    "    x_q = quartz_lastdim(x, group_size, lzs_block_size, mode=mode)\n",
    "    w_q = quartz_lastdim(w, group_size, lzs_block_size, mode=mode)\n",
    "    y_q = x_q @ w_q\n",
    "\n",
    "    return torch.norm(y_ref - y_q, p=\"fro\").item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838eaa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_sizes = [1, 4, 8, 16, 32, 64, 128]\n",
    "\n",
    "y_org = x_org @ w_org\n",
    "y_rs = x_rs @ w_rs\n",
    "\n",
    "errors_org = []\n",
    "errors_rs = []\n",
    "\n",
    "for g in group_sizes:\n",
    "    lzs_block = min(16, g)\n",
    "    errors_org.append(quartz_matmul_error(x_org, w_org, g, lzs_block_size=lzs_block, y_ref=y_org))\n",
    "    errors_rs.append(quartz_matmul_error(x_rs, w_rs, g, lzs_block_size=lzs_block, y_ref=y_rs))\n",
    "\n",
    "errors_org, errors_rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eee2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "colors = {\"org\": \"#0072B2\", \"rs\": \"#D55E00\"}  # Okabe-Ito palette\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.2, 4.4), dpi=140)\n",
    "ax.plot(group_sizes, errors_org, marker=\"o\", lw=2.2, color=colors[\"org\"], label=\"No RS\")\n",
    "ax.plot(group_sizes, errors_rs, marker=\"s\", lw=2.2, ls=\"--\", color=colors[\"rs\"], label=\"RS\")\n",
    "\n",
    "ax.set_xscale(\"log\", base=2)\n",
    "ax.set_xticks(group_sizes)\n",
    "ax.xaxis.set_major_formatter(mticker.ScalarFormatter())\n",
    "ax.xaxis.set_minor_formatter(mticker.NullFormatter())\n",
    "\n",
    "ax.set_xlabel(\"Group size\")\n",
    "ax.set_ylabel(r\"$\\|XW - Q(X)Q(W)\\|_F$\")\n",
    "ax.set_title(\"QUARTZ quantization error vs group size\")\n",
    "ax.grid(True, axis=\"y\", linestyle=\"--\", alpha=0.45)\n",
    "ax.legend(frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438416c",
   "metadata": {},
   "source": [
    "### Smoothing scale and alpha grid search (nvfp4 extractions)\n",
    "\n",
    "These helpers compute a per-input-channel smooth scale from input activations and weights\n",
    "at transformer_blocks.11.attn.to_q, then run a simple alpha grid search using QUARTZ\n",
    "quantization to pick the alpha that minimizes ||XW - Q(X)Q(W)||_F. Reverse smoothing\n",
    "uses the inverse scaling (X * s, W / s) but shares the same scale formula.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ded4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "layer_name = \"transformer_blocks.11.attn.to_q\"\n",
    "\n",
    "if \"act_org\" not in globals():\n",
    "    act_org = torch.load(\n",
    "        \"final_result/nvfp4/extractions/activations_during_generation.pt\",\n",
    "        weights_only=False,\n",
    "    )\n",
    "if \"w_org\" not in globals():\n",
    "    w_org = torch.load(\n",
    "        \"final_result/nvfp4/extractions/weights_before_smooth.pt\",\n",
    "        weights_only=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_layer_inputs(\n",
    "    act_cache,\n",
    "    layer,\n",
    "    dataset=\"MJHQ\",\n",
    "    timestep=0,\n",
    "    max_samples=None,\n",
    "):\n",
    "    key = f\"{layer}.input\"\n",
    "    raw = act_cache[key][-1]\n",
    "    selected = []\n",
    "    for i, tensor in enumerate(raw):\n",
    "        dset = \"MJHQ\" if i < 64 else \"sDCI\"\n",
    "        t = i % 4\n",
    "        if dset == dataset and t == timestep:\n",
    "            selected.append(tensor)\n",
    "            if max_samples is not None and len(selected) >= max_samples:\n",
    "                break\n",
    "    if not selected:\n",
    "        raise ValueError(f\"No activations found for dataset={dataset}, timestep={timestep}\")\n",
    "    return torch.stack(selected, dim=0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def flatten_acts(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.reshape(-1, x.shape[-1])\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_span(x: torch.Tensor, mode: str = \"abs_max\") -> torch.Tensor:\n",
    "    if mode == \"abs_max\":\n",
    "        return x.abs().amax(dim=0)\n",
    "    if mode == \"rms\":\n",
    "        return x.pow(2).mean(dim=0).sqrt()\n",
    "    raise ValueError(f\"Unknown span mode: {mode}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_scale_from_acts_weights(\n",
    "    x: torch.Tensor,\n",
    "    w: torch.Tensor,\n",
    "    alpha: float,\n",
    "    beta: float,\n",
    "    span_mode: str = \"abs_max\",\n",
    "    eps: float = 1e-12,\n",
    ") -> torch.Tensor:\n",
    "    x_span = compute_span(x, mode=span_mode)\n",
    "    w_span = compute_span(w, mode=span_mode)\n",
    "    scale = (x_span.clamp_min(eps).pow(alpha) / w_span.clamp_min(eps).pow(beta))\n",
    "    return scale.nan_to_num(1.0, posinf=1.0, neginf=1.0)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_smoothing(x: torch.Tensor, w: torch.Tensor, scale: torch.Tensor):\n",
    "    scale_x = scale.view(1, -1)\n",
    "    x_sm = x / scale_x\n",
    "    w_sm = w * scale\n",
    "    return x_sm, w_sm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def apply_reverse_smoothing(x: torch.Tensor, w: torch.Tensor, scale: torch.Tensor):\n",
    "    scale_x = scale.view(1, -1)\n",
    "    x_sm = x * scale_x\n",
    "    w_sm = w / scale\n",
    "    return x_sm, w_sm\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def smooth_scale_for_layer(\n",
    "    act_cache,\n",
    "    weight_cache,\n",
    "    layer,\n",
    "    *,\n",
    "    dataset=\"MJHQ\",\n",
    "    timestep=0,\n",
    "    max_samples=None,\n",
    "    token_stride=32,\n",
    "    alpha=0.5,\n",
    "    beta_mode=\"one_minus_alpha\",\n",
    "    span_mode=\"abs_max\",\n",
    "):\n",
    "    inputs = extract_layer_inputs(\n",
    "        act_cache, layer, dataset=dataset, timestep=timestep, max_samples=max_samples\n",
    "    )\n",
    "    inputs = inputs[:, 0]\n",
    "    inputs = inputs[:, ::token_stride, :]\n",
    "    x = flatten_acts(inputs).float()\n",
    "    w = weight_cache[layer][\"weight\"].float()\n",
    "    beta = 1.0 - alpha if beta_mode == \"one_minus_alpha\" else float(beta_mode)\n",
    "    scale = smooth_scale_from_acts_weights(x, w, alpha=alpha, beta=beta, span_mode=span_mode)\n",
    "    return scale, x, w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf6246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def grid_search_alpha(\n",
    "    x: torch.Tensor,\n",
    "    w: torch.Tensor,\n",
    "    *,\n",
    "    mode: str = \"smooth\",\n",
    "    span_mode: str = \"abs_max\",\n",
    "    num_grids: int = 20,\n",
    "    alpha_candidates=None,\n",
    "    beta_mode=\"one_minus_alpha\",\n",
    "    quant_block_size: int = 32,\n",
    "    lzs_block_size: int = 16,\n",
    "):\n",
    "    if alpha_candidates is None:\n",
    "        if mode == \"reverse\":\n",
    "            alpha_candidates = [i / num_grids for i in range(-num_grids, num_grids + 1) if i != 0]\n",
    "        else:\n",
    "            alpha_candidates = [i / num_grids for i in range(1, num_grids + 1)]\n",
    "\n",
    "    y_ref = x @ w.t()\n",
    "    results = []\n",
    "    for alpha in alpha_candidates:\n",
    "        beta = 1.0 - alpha if beta_mode == \"one_minus_alpha\" else float(beta_mode)\n",
    "        scale = smooth_scale_from_acts_weights(x, w, alpha=alpha, beta=beta, span_mode=span_mode)\n",
    "        if mode == \"reverse\":\n",
    "            x_sm, w_sm = apply_reverse_smoothing(x, w, scale)\n",
    "        elif mode == \"smooth\":\n",
    "            x_sm, w_sm = apply_smoothing(x, w, scale)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "        x_q = quartz_lastdim(x_sm, quant_block_size, lzs_block_size, mode=\"lzs\")\n",
    "        w_q = quartz_lastdim(w_sm, quant_block_size, lzs_block_size, mode=\"lzs\")\n",
    "        err = torch.norm(y_ref - (x_q @ w_q.t()), p=\"fro\").item()\n",
    "        results.append((alpha, err))\n",
    "    best = min(results, key=lambda it: it[1])\n",
    "    return results, best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77d65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, x_flat, w_mat = smooth_scale_for_layer(\n",
    "    act_org,\n",
    "    w_org,\n",
    "    layer_name,\n",
    "    dataset=\"MJHQ\",\n",
    "    timestep=0,\n",
    "    max_samples=8,\n",
    "    token_stride=32,\n",
    "    alpha=0.5,\n",
    "    beta_mode=\"one_minus_alpha\",\n",
    "    span_mode=\"abs_max\",\n",
    ")\n",
    "\n",
    "x_sm, w_sm = apply_smoothing(x_flat, w_mat, scale)\n",
    "x_rev, w_rev = apply_reverse_smoothing(x_flat, w_mat, scale)\n",
    "\n",
    "scale.min().item(), scale.max().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77464134",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_results, smooth_best = grid_search_alpha(\n",
    "    x_flat,\n",
    "    w_mat,\n",
    "    mode=\"smooth\",\n",
    "    span_mode=\"abs_max\",\n",
    "    num_grids=20,\n",
    "    beta_mode=\"one_minus_alpha\",\n",
    "    quant_block_size=32,\n",
    "    lzs_block_size=16,\n",
    ")\n",
    "\n",
    "reverse_results, reverse_best = grid_search_alpha(\n",
    "    x_flat,\n",
    "    w_mat,\n",
    "    mode=\"reverse\",\n",
    "    span_mode=\"abs_max\",\n",
    "    num_grids=20,\n",
    "    beta_mode=\"one_minus_alpha\",\n",
    "    quant_block_size=32,\n",
    "    lzs_block_size=16,\n",
    ")\n",
    "\n",
    "smooth_best, reverse_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdde3fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import Transformer2DModel, PixArtSigmaPipeline\n",
    "\n",
    "# Set fixed seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weight_dtype = torch.float16\n",
    "\n",
    "pipe = PixArtSigmaPipeline.from_pretrained(\n",
    "    \"PixArt-alpha/PixArt-Sigma-XL-2-1024-MS\", \n",
    "    torch_dtype=weight_dtype,\n",
    "    use_safetensors=True,\n",
    ")\n",
    "pipe = pipe.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce6d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepcompressor.app.diffusion.investigate import ActivationModifier, ActivationExtractor\n",
    "\n",
    "# Define skip patterns based on the YAML config\n",
    "skip_patterns = [\n",
    "    \"embed\",              # embedding layers\n",
    "    \"proj_in\",            # transformer_proj_in\n",
    "    \"proj_out\",           # transformer_proj_out\n",
    "    \"adaln_single\",       # transformer_norm / transformer_add_norm\n",
    "    \"caption_projection\", # embed\n",
    "    \"time_embed\",         # resblock_time_proj\n",
    "]\n",
    "\n",
    "def get_allowed_layers(model, skip_patterns: list[str]) -> set[str]:\n",
    "    \"\"\"Get all Linear layer names except those matching skip patterns.\"\"\"\n",
    "    allowed = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Check if this layer should be skipped\n",
    "            should_skip = any(pattern in name for pattern in skip_patterns)\n",
    "            if not should_skip:\n",
    "                allowed.add(name)\n",
    "    return allowed\n",
    "\n",
    "# Get the transformer model from the pipeline\n",
    "transformer = pipe.transformer\n",
    "\n",
    "# Get allowed layers (all Linear layers except skipped ones)\n",
    "allowed_layers = get_allowed_layers(transformer, skip_patterns)\n",
    "allowed_layers = [name for name in allowed_layers if 'transformer_blocks.0' in name]\n",
    "\n",
    "print(f\"Total allowed layers: {len(allowed_layers)}\")\n",
    "print(\"Allowed layers:\", sorted(allowed_layers)[:10], \"...\")  # Print first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba980ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register the modifier\n",
    "torch.manual_seed(12345)\n",
    "prompt = \"ultra realistic, summer, jungle, uranian blue, ice blue, light green, melon, salmon pink, flowers, leaves, white background \"\n",
    "# image = pipe(prompt).images[0]\n",
    "# image.save(f\"./figs/tau_removed/fp.png\")\n",
    "extractor = ActivationExtractor(\n",
    "    transformer,\n",
    "    allowed_layers=allowed_layers,\n",
    ")\n",
    "extractor.register_hooks()\n",
    "\n",
    "# Run inference with modified activations\n",
    "image = pipe(prompt).images[0]\n",
    "extractor.save_activations(suffix=\"pixart\")\n",
    "extractor.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5495753",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor.save_activations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and register the modifier\n",
    "torch.manual_seed(12345)\n",
    "prompt = \"ultra realistic, summer, jungle, uranian blue, ice blue, light green, melon, salmon pink, flowers, leaves, white background \"\n",
    "# image = pipe(prompt).images[0]\n",
    "# image.save(f\"./figs/tau_removed/fp.png\")\n",
    "for ratio in [0.25]:\n",
    "    torch.manual_seed(12345)\n",
    "    modifier = ActivationModifier(\n",
    "        transformer,\n",
    "        allowed_layers=allowed_layers,\n",
    "        zero_small_activations=True,\n",
    "        small_threshold_ratio=ratio,\n",
    "        mode=\"oneminustau\",\n",
    "    )\n",
    "    modifier.register_hooks()\n",
    "\n",
    "    # Run inference with modified activations\n",
    "    image = pipe(prompt).images[0]\n",
    "    image.save(f\"./figs/tau_removed/outlier_{1-ratio}.png\")\n",
    "\n",
    "    # Clean up hooks after inference\n",
    "    modifier.remove_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d064168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "base_path = \"/home/work/workspace/dhkim2810/qtbench/examples/diffusion/baselines/torch.bfloat16/flux.1-schnell/fmeuler4-g0/samples\"\n",
    "target_path = \"/home/work/workspace/dhkim2810/qtbench/examples/diffusion/runs/diffusion/flux.1/flux.1-schnell/w.4-x.4-y.16/w.sint4-x.sint4.u-y.bf16/w.v32.bf16-x.v32.bf16-y.tnsr.bf16/w.static/shift-skip.x.[[w]+tan+tn].w.[e+rs+rtp+s+tpi+tpo]-qdiff.128-t4.g0-s5000/no_inliner_1/samples\"\n",
    "\n",
    "dset = \"MJHQ\" # or \"MJHQ\"\n",
    "if dset == \"MJHQ\":\n",
    "    base_path += \"/MJHQ/MJHQ-5000\"\n",
    "    target_path += \"/MJHQ/MJHQ-16\"\n",
    "else:\n",
    "    base_path += \"/DCI/sDCI-5000\"\n",
    "    target_path += \"/DCI/sDCI-16\"\n",
    "\n",
    "base_imgs = os.listdir(base_path)\n",
    "filtered_imgs = os.listdir(target_path)\n",
    "\n",
    "# Find common images in both directories\n",
    "common_imgs = set(base_imgs).intersection(set(filtered_imgs))\n",
    "print(f\"Found {len(common_imgs)} common images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b354b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 0\n",
    "tau = 2\n",
    "target_path = target_path.replace(\"no_inliner_1\", f\"no_inliner_{tau}\")\n",
    "# Show images side by side\n",
    "for img_name in sorted(common_imgs):\n",
    "    base_img = Image.open(os.path.join(base_path, img_name))\n",
    "    target_img = Image.open(os.path.join(target_path, img_name))\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(base_img)\n",
    "    axes[0].set_title(\"Base Image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(target_img)\n",
    "    axes[1].set_title(\"Target Image\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Image: {img_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    img_idx += 1\n",
    "    if img_idx >= 5:  # Display only first 5 images for brevity\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18326fe",
   "metadata": {},
   "source": [
    "# Quantization Type Case Study\n",
    "\n",
    "1. SINT4\n",
    "2. UINT4\n",
    "3. CINT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple\n",
    "\n",
    "def qint8(x: torch.Tensor, s: torch.Tensor | None, z: torch.Tensor | None, qtype: str = \"sym\") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if qtype == \"sym\":\n",
    "        s = x.abs().max() / 127.0\n",
    "        z = torch.zeros_like(s)\n",
    "        qmin, qmax = -128, 127\n",
    "    elif qtype == \"asym\":\n",
    "        s = (x.max() - x.min()) / 255.0\n",
    "        z = torch.round(-x.min() / s)\n",
    "        qmin, qmax = 0, 255\n",
    "    elif qtype == \"custom\":\n",
    "        s = (x.max() - x.min()) / 128.0\n",
    "        z = torch.zeros_like(s)\n",
    "        qmin, qmax = -128, 127\n",
    "\n",
    "    x_scaled = x / s\n",
    "    x_rounded = torch.round(x_scaled) + z\n",
    "    x_clamped = x_rounded.clamp(qmin, qmax)\n",
    "    x_dequant = (x_clamped - z) * s\n",
    "\n",
    "    return x_dequant, x_rounded\n",
    "\n",
    "q_err = lambda x, x_dq : torch.norm(x - x_dq, p='fro').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff510b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(128)\n",
    "s = None\n",
    "z = None\n",
    "\n",
    "target_var = 0.1\n",
    "target_mean = 2.1\n",
    "\n",
    "x = x * (target_var / x.var()).sqrt()\n",
    "x = x - x.mean() + target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f415d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X mean/var before quant: {x.mean().item():.6f} / {x.var().item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7aba11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dq_sym, x_int_sym = qint8(x, s, z, qtype=\"sym\")\n",
    "q_err(x, x_dq_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ff80e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dq_asym, x_int_asym = qint8(x, s, z, qtype=\"asym\")\n",
    "q_err(x, x_dq_asym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502d0da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dq_c, x_int_c = qint8(x, s, z, qtype=\"custom\")\n",
    "q_err(x, x_dq_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a4de68",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_int_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51cccea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_density, bins_density = torch.histogram(x, bins=256, range=(-4, 4), density=True)\n",
    "x_dq_sym_density, _ = torch.histogram(x_dq_sym, bins=256, range=(-4, 4), density=True)\n",
    "x_dq_asym_density, _ = torch.histogram(x_dq_asym, bins=256, range=(-4, 4), density=True)\n",
    "x_dq_c_density, _ = torch.histogram(x_dq_c, bins=256, range=(-4, 4), density=True)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(bins_density[:-1].numpy(), x_density.numpy(), width=(bins_density[1]-bins_density[0]).item(), alpha=0.5, label='Original', color='blue')\n",
    "plt.bar(bins_density[:-1].numpy(), x_dq_sym_density.numpy(), width=(bins_density[1]-bins_density[0]).item(), alpha=0.5, label='Quantized Sym', color='orange')\n",
    "plt.bar(bins_density[:-1].numpy(), x_dq_asym_density.numpy(), width=(bins_density[1]-bins_density[0]).item(), alpha=0.5, label='Quantized Asym', color='green')\n",
    "# plt.bar(bins_density[:-1].numpy(), x_dq_c_density.numpy(), width=(bins_density[1]-bins_density[0]).item(), alpha=0.5, label='Quantized Custom', color='red')\n",
    "plt.legend()\n",
    "plt.title('Histogram of Original and Quantized Values')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0719f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.random.randint(0, 32, size=(32,), dtype=np.uint8)\n",
    "x_reduce = np.bitwise_or.reduce(x)\n",
    "flag = max(np.floor(np.log2(x_reduce)) - 3, np.uint8(0)).astype(np.uint8)\n",
    "print(x_reduce, flag)\n",
    "x_compressed = x >> flag\n",
    "x, x_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f81ff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.uint8(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14271c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad1f081b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transformer_blocks.0.attn1.to_q', 'transformer_blocks.0.attn1.to_k', 'transformer_blocks.0.attn1.to_v', 'transformer_blocks.0.attn1.to_out.0', 'transformer_blocks.0.attn2.to_q', 'transformer_blocks.0.attn2.to_k', 'transformer_blocks.0.attn2.to_v', 'transformer_blocks.0.attn2.to_out.0', 'transformer_blocks.0.ff.net.0.proj', 'transformer_blocks.0.ff.net.2'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105d4cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0289262a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4096, 1152])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09286f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0085, dtype=torch.float16),\n",
       " tensor(0.0368, dtype=torch.float16),\n",
       " tensor(-3.1660, dtype=torch.float16),\n",
       " tensor(2.7305, dtype=torch.float16))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt[0].mean(), xt[0].var(), xt[0].min(), xt[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d27772c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0d9160",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qtbench-rq0DrYIM-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
